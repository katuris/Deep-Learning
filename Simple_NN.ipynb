{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple NN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmj9J4CBd6Ls"
      },
      "source": [
        "#IMPORT FILES FROM DRIVE INTO GOOGLE-COLAB:\n",
        "\n",
        "#STEP-1: Import Libraries\n",
        "\n",
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from sklearn.metrics import auc\n",
        "#STEP-2: Autheticate E-Mail ID\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#STEP-3: Get File from Drive using file-ID\n",
        "\n",
        "#STEP-3: Get File from Drive using file-ID\n",
        "#https://drive.google.com/file/d/1bjXXLoFetcMnVMX3VK94C4Ejz_FPY6UA/view?usp=sharing\n",
        "#2.1 Get the file\n",
        "downloaded = drive.CreateFile({'id':'1bjXXLoFetcMnVMX3VK94C4Ejz_FPY6UA'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('CellDNAWithHeaders.csv') \n",
        "\n",
        "#2.1 Get the file\n",
        "#downloaded = drive.CreateFile({'id':'1SOcTxkqcyCxw-SX-stSnRk86DB1SUytw'}) # replace the id with id of file you want to access\n",
        "#downloaded.GetContentFile('CellDNA.csv') \n",
        "\n",
        "\n",
        "\n",
        "#accuracy on the test dataset\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "#cm = confusion_matrix(y_test, y_pred)\n",
        "#print(cm)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKj3zeCOgJLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "5b13bc9d-5dc2-45ef-b760-aa5d34abcc46"
      },
      "source": [
        "pip install category_encoders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 61kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 71kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 81kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 92kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.18.3)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.1)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MpvlPikf1A0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f471b509-e53c-4cbd-8566-4da57f659c82"
      },
      "source": [
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve ,auc\n",
        "import category_encoders as ce\n",
        "import numpy as np\n",
        " \n",
        "\n",
        "df = pd.read_csv(\"CellDNAWithHeaders.csv\")\n",
        "print(list(df.columns.values))\n",
        "\n",
        "#create dummy variables for the last column\n",
        "df = pd.get_dummies(df,columns=[\"Col 14\"])\n",
        "print(list(df.columns.values))\n",
        "\n",
        "# creating input features and target variables\n",
        "X = df.iloc[:,:13].values\n",
        "y = df.iloc[:,13:24].values\n",
        "print(list(X[0]))\n",
        "print(list(y[0]))\n",
        "n_classes  = y.shape[1]\n",
        "\n",
        "print(n_classes)\n",
        "#Splitting the dataset into X_train,X_test,y_train,y_test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "#standardizing the input feature\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "#Splitting the dataset into X_train,X_test,y_train,y_test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "classifier = Sequential()\n",
        "#First Hidden Layer\n",
        "classifier.add(Dense(9, activation='relu', kernel_initializer='random_normal', input_dim=13))\n",
        "#Second  Hidden Layer\n",
        "classifier.add(Dense(7, activation='relu', kernel_initializer='random_normal'))\n",
        "#third  Hidden Layer\n",
        "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
        "#Output Layer\n",
        "classifier.add(Dense(11, activation='sigmoid', kernel_initializer='random_normal'))\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "#calculating recall\n",
        "def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "#calculating precision\n",
        "def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "# compiling  the neural network\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',precision, recall])\n",
        "\n",
        "#Fitting the data to the training dataset\n",
        "y_score = classifier.fit(X_train,y_train, batch_size=10, epochs=70)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print(y_score.history)\n",
        "classifier.summary()\n",
        "\n",
        "#compute ROC\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(n_classes):\n",
        "  print(y_test[:,i].shape)\n",
        "  print(np.array(y_pred).shape)\n",
        "\n",
        "  fpr[i], tpr[i] , _ =  roc_curve(y_test[:,i], y_pred[:,i])\n",
        "  roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "  \n",
        "\n",
        "#plot the graph\n",
        "plt.figure()\n",
        "plt.plot(fpr[2],tpr[2], color='blue',lw=2, label='ROC curve (area = %.2f)' % roc_auc[2])\n",
        "\n",
        "#eval_model=classifier.evaluate(X_train, y_train)\n",
        "#eval_model\n",
        "#predict the output for our test dataset\n",
        "#y_pred=classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Col 1', 'Col 2', 'Col 3', 'Col 4', 'Col 5', 'Col 6', 'Col 7', 'Col 8', 'Col 9', 'Col 10', 'Col 11', 'Col 12', 'Col 13', 'Col 14']\n",
            "['Col 1', 'Col 2', 'Col 3', 'Col 4', 'Col 5', 'Col 6', 'Col 7', 'Col 8', 'Col 9', 'Col 10', 'Col 11', 'Col 12', 'Col 13', 'Col 14_0', 'Col 14_1', 'Col 14_2', 'Col 14_3', 'Col 14_4', 'Col 14_5', 'Col 14_6', 'Col 14_7', 'Col 14_8', 'Col 14_9', 'Col 14_10']\n",
            "[222.0, 31.18918919, 40.34234234, 35.57908668, 8.883916969, 0.968324558, -80.11367302, 222.0, 1.0, 16.81247093, 0.8161764709999999, 0.578125, 78.59100000000001]\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "11\n",
            "Epoch 1/70\n",
            "851/851 [==============================] - 0s 499us/step - loss: 0.6520 - acc: 0.9666 - precision: 0.8313 - recall: 0.8395\n",
            "Epoch 2/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.3192 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 3/70\n",
            "851/851 [==============================] - 0s 116us/step - loss: 0.1360 - acc: 0.9705 - precision: 0.8291 - recall: 0.8291\n",
            "Epoch 4/70\n",
            "851/851 [==============================] - 0s 124us/step - loss: 0.1123 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 5/70\n",
            "851/851 [==============================] - 0s 119us/step - loss: 0.1036 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 6/70\n",
            "851/851 [==============================] - 0s 119us/step - loss: 0.0982 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 7/70\n",
            "851/851 [==============================] - 0s 120us/step - loss: 0.0946 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 8/70\n",
            "851/851 [==============================] - 0s 121us/step - loss: 0.0921 - acc: 0.9705 - precision: 0.8291 - recall: 0.8291\n",
            "Epoch 9/70\n",
            "851/851 [==============================] - 0s 121us/step - loss: 0.0902 - acc: 0.9705 - precision: 0.8291 - recall: 0.8291\n",
            "Epoch 10/70\n",
            "851/851 [==============================] - 0s 130us/step - loss: 0.0888 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 11/70\n",
            "851/851 [==============================] - 0s 138us/step - loss: 0.0876 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 12/70\n",
            "851/851 [==============================] - 0s 130us/step - loss: 0.0866 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 13/70\n",
            "851/851 [==============================] - 0s 127us/step - loss: 0.0857 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 14/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.0849 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 15/70\n",
            "851/851 [==============================] - 0s 133us/step - loss: 0.0841 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 16/70\n",
            "851/851 [==============================] - 0s 128us/step - loss: 0.0835 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 17/70\n",
            "851/851 [==============================] - 0s 125us/step - loss: 0.0828 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 18/70\n",
            "851/851 [==============================] - 0s 120us/step - loss: 0.0824 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 19/70\n",
            "851/851 [==============================] - 0s 116us/step - loss: 0.0819 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 20/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.0813 - acc: 0.9705 - precision: 0.8291 - recall: 0.8291\n",
            "Epoch 21/70\n",
            "851/851 [==============================] - 0s 137us/step - loss: 0.0811 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 22/70\n",
            "851/851 [==============================] - 0s 115us/step - loss: 0.0806 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 23/70\n",
            "851/851 [==============================] - 0s 144us/step - loss: 0.0802 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 24/70\n",
            "851/851 [==============================] - 0s 147us/step - loss: 0.0795 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 25/70\n",
            "851/851 [==============================] - 0s 145us/step - loss: 0.0791 - acc: 0.9705 - precision: 0.8291 - recall: 0.8291\n",
            "Epoch 26/70\n",
            "851/851 [==============================] - 0s 152us/step - loss: 0.0787 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 27/70\n",
            "851/851 [==============================] - 0s 147us/step - loss: 0.0783 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 28/70\n",
            "851/851 [==============================] - 0s 123us/step - loss: 0.0779 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 29/70\n",
            "851/851 [==============================] - 0s 134us/step - loss: 0.0777 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 30/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.0772 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 31/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.0770 - acc: 0.9705 - precision: 0.8291 - recall: 0.8291\n",
            "Epoch 32/70\n",
            "851/851 [==============================] - 0s 143us/step - loss: 0.0769 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 33/70\n",
            "851/851 [==============================] - 0s 142us/step - loss: 0.0763 - acc: 0.9705 - precision: 0.8395 - recall: 0.8395\n",
            "Epoch 34/70\n",
            "851/851 [==============================] - 0s 153us/step - loss: 0.0763 - acc: 0.9713 - precision: 0.8472 - recall: 0.8395\n",
            "Epoch 35/70\n",
            "851/851 [==============================] - 0s 162us/step - loss: 0.0756 - acc: 0.9735 - precision: 0.8673 - recall: 0.8395\n",
            "Epoch 36/70\n",
            "851/851 [==============================] - 0s 155us/step - loss: 0.0755 - acc: 0.9731 - precision: 0.8637 - recall: 0.8395\n",
            "Epoch 37/70\n",
            "851/851 [==============================] - 0s 143us/step - loss: 0.0752 - acc: 0.9751 - precision: 0.8847 - recall: 0.8384\n",
            "Epoch 38/70\n",
            "851/851 [==============================] - 0s 142us/step - loss: 0.0748 - acc: 0.9754 - precision: 0.8896 - recall: 0.8372\n",
            "Epoch 39/70\n",
            "851/851 [==============================] - 0s 141us/step - loss: 0.0747 - acc: 0.9754 - precision: 0.8887 - recall: 0.8372\n",
            "Epoch 40/70\n",
            "851/851 [==============================] - 0s 139us/step - loss: 0.0743 - acc: 0.9770 - precision: 0.8980 - recall: 0.8244\n",
            "Epoch 41/70\n",
            "851/851 [==============================] - 0s 144us/step - loss: 0.0742 - acc: 0.9767 - precision: 0.8935 - recall: 0.8221\n",
            "Epoch 42/70\n",
            "851/851 [==============================] - 0s 145us/step - loss: 0.0740 - acc: 0.9776 - precision: 0.9171 - recall: 0.8314\n",
            "Epoch 43/70\n",
            "851/851 [==============================] - 0s 147us/step - loss: 0.0737 - acc: 0.9776 - precision: 0.9159 - recall: 0.8326\n",
            "Epoch 44/70\n",
            "851/851 [==============================] - 0s 138us/step - loss: 0.0734 - acc: 0.9777 - precision: 0.9170 - recall: 0.8314\n",
            "Epoch 45/70\n",
            "851/851 [==============================] - 0s 119us/step - loss: 0.0734 - acc: 0.9780 - precision: 0.9230 - recall: 0.8291\n",
            "Epoch 46/70\n",
            "851/851 [==============================] - 0s 118us/step - loss: 0.0731 - acc: 0.9779 - precision: 0.9233 - recall: 0.8279\n",
            "Epoch 47/70\n",
            "851/851 [==============================] - 0s 131us/step - loss: 0.0730 - acc: 0.9779 - precision: 0.9248 - recall: 0.8256\n",
            "Epoch 48/70\n",
            "851/851 [==============================] - 0s 115us/step - loss: 0.0727 - acc: 0.9778 - precision: 0.9283 - recall: 0.8244\n",
            "Epoch 49/70\n",
            "851/851 [==============================] - 0s 118us/step - loss: 0.0726 - acc: 0.9783 - precision: 0.9302 - recall: 0.8279\n",
            "Epoch 50/70\n",
            "851/851 [==============================] - 0s 115us/step - loss: 0.0724 - acc: 0.9782 - precision: 0.9193 - recall: 0.8140\n",
            "Epoch 51/70\n",
            "851/851 [==============================] - 0s 121us/step - loss: 0.0722 - acc: 0.9783 - precision: 0.9308 - recall: 0.8267\n",
            "Epoch 52/70\n",
            "851/851 [==============================] - 0s 134us/step - loss: 0.0720 - acc: 0.9784 - precision: 0.9366 - recall: 0.8209\n",
            "Epoch 53/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.0719 - acc: 0.9786 - precision: 0.9343 - recall: 0.8256\n",
            "Epoch 54/70\n",
            "851/851 [==============================] - 0s 143us/step - loss: 0.0717 - acc: 0.9785 - precision: 0.9369 - recall: 0.8221\n",
            "Epoch 55/70\n",
            "851/851 [==============================] - 0s 129us/step - loss: 0.0716 - acc: 0.9783 - precision: 0.9338 - recall: 0.8221\n",
            "Epoch 56/70\n",
            "851/851 [==============================] - 0s 144us/step - loss: 0.0715 - acc: 0.9786 - precision: 0.9364 - recall: 0.8221\n",
            "Epoch 57/70\n",
            "851/851 [==============================] - 0s 132us/step - loss: 0.0712 - acc: 0.9785 - precision: 0.9344 - recall: 0.8209\n",
            "Epoch 58/70\n",
            "851/851 [==============================] - 0s 119us/step - loss: 0.0711 - acc: 0.9786 - precision: 0.9323 - recall: 0.8233\n",
            "Epoch 59/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.0709 - acc: 0.9787 - precision: 0.9301 - recall: 0.8093\n",
            "Epoch 60/70\n",
            "851/851 [==============================] - 0s 126us/step - loss: 0.0712 - acc: 0.9794 - precision: 0.9571 - recall: 0.8140\n",
            "Epoch 61/70\n",
            "851/851 [==============================] - 0s 144us/step - loss: 0.0708 - acc: 0.9787 - precision: 0.9418 - recall: 0.8186\n",
            "Epoch 62/70\n",
            "851/851 [==============================] - 0s 143us/step - loss: 0.0706 - acc: 0.9786 - precision: 0.9298 - recall: 0.8093\n",
            "Epoch 63/70\n",
            "851/851 [==============================] - 0s 137us/step - loss: 0.0705 - acc: 0.9785 - precision: 0.9407 - recall: 0.8186\n",
            "Epoch 64/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.0703 - acc: 0.9790 - precision: 0.9430 - recall: 0.8221\n",
            "Epoch 65/70\n",
            "851/851 [==============================] - 0s 116us/step - loss: 0.0702 - acc: 0.9791 - precision: 0.9471 - recall: 0.8174\n",
            "Epoch 66/70\n",
            "851/851 [==============================] - 0s 116us/step - loss: 0.0700 - acc: 0.9790 - precision: 0.9372 - recall: 0.8047\n",
            "Epoch 67/70\n",
            "851/851 [==============================] - 0s 119us/step - loss: 0.0701 - acc: 0.9791 - precision: 0.9519 - recall: 0.8128\n",
            "Epoch 68/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.0699 - acc: 0.9791 - precision: 0.9510 - recall: 0.8128\n",
            "Epoch 69/70\n",
            "851/851 [==============================] - 0s 117us/step - loss: 0.0697 - acc: 0.9788 - precision: 0.9500 - recall: 0.8128\n",
            "Epoch 70/70\n",
            "851/851 [==============================] - 0s 130us/step - loss: 0.0696 - acc: 0.9790 - precision: 0.9404 - recall: 0.8023\n",
            "{'loss': [0.6519829805953522, 0.31916905333617424, 0.135961270242159, 0.11233924294014261, 0.10363258487084218, 0.0982280012644766, 0.09461920514164358, 0.09205723847191426, 0.09018169261283131, 0.08876263516789962, 0.08760883749903702, 0.08658882783622986, 0.08570482507216734, 0.08488282978660006, 0.08410126776418066, 0.08351674909374024, 0.08284099080988899, 0.0824104841447246, 0.08188928787957124, 0.08130692018001186, 0.08112449300588688, 0.08060328198047453, 0.08019870561265773, 0.07950620860086817, 0.07909184188902658, 0.078659646043972, 0.07826183010144411, 0.0779216151050326, 0.07767805835659052, 0.0772434003069852, 0.0769787902405352, 0.07690008936890923, 0.07630732679328105, 0.0762598110684295, 0.0756204229370241, 0.075452485577872, 0.07523036783752081, 0.07479669916518492, 0.0746744483449145, 0.07429681240431045, 0.07419425037951613, 0.07402679787800438, 0.07370663067936258, 0.07342141096294282, 0.07341396033427841, 0.0730607217532629, 0.07296534890547553, 0.07273504103482932, 0.072636854834923, 0.07236743161671486, 0.07222700230636415, 0.07195275889796596, 0.07192629528464745, 0.07171127248213803, 0.07161274187256148, 0.07146135244944722, 0.07119803302297975, 0.07110909956641995, 0.07092568163060146, 0.07124334689009272, 0.07079266080497101, 0.07063260190320211, 0.07048340896520021, 0.07030423554417817, 0.07024974420006447, 0.07002636648894568, 0.0700619958826943, 0.06985911906680731, 0.06972400218431485, 0.06962079434747193], 'acc': [0.9665629, 0.9705155, 0.97051543, 0.9705155, 0.97051567, 0.97051555, 0.97051555, 0.9705155, 0.97051555, 0.97051567, 0.97051555, 0.97051543, 0.97051567, 0.97051555, 0.97051555, 0.97051567, 0.9705155, 0.9705155, 0.97051555, 0.9705155, 0.97051555, 0.97051555, 0.97051555, 0.97051555, 0.97051555, 0.9705155, 0.9705155, 0.97051567, 0.97051555, 0.9705155, 0.97051555, 0.97051555, 0.97051555, 0.9712636, 0.9735071, 0.9730796, 0.97510964, 0.97542995, 0.9754299, 0.97703266, 0.9767121, 0.9775666, 0.97756684, 0.9776736, 0.97799397, 0.977887, 0.977887, 0.9777803, 0.97831446, 0.97820765, 0.97831446, 0.97842133, 0.978635, 0.97852826, 0.97831446, 0.9786348, 0.9785282, 0.97863495, 0.97874177, 0.97938275, 0.97874165, 0.97863495, 0.9785282, 0.9789556, 0.9790622, 0.97895557, 0.9790624, 0.97906226, 0.97884876, 0.97895545], 'precision': [0.8312965, 0.8395348, 0.82906973, 0.8395351, 0.8395352, 0.839535, 0.839535, 0.8290699, 0.82907, 0.839535, 0.839535, 0.8395351, 0.8395351, 0.8395351, 0.8395351, 0.83953494, 0.83953494, 0.8395348, 0.839535, 0.8290698, 0.8395351, 0.839535, 0.83953494, 0.83953494, 0.8290699, 0.83953494, 0.83953494, 0.83953494, 0.8395351, 0.8395351, 0.8290698, 0.839535, 0.8395351, 0.8471578, 0.8673128, 0.8636675, 0.8846762, 0.8896364, 0.8886952, 0.89795154, 0.8935403, 0.91709137, 0.9158823, 0.9169621, 0.9230022, 0.92331135, 0.9248108, 0.9283316, 0.9301679, 0.91932446, 0.9307863, 0.9366048, 0.9343438, 0.93691397, 0.93382704, 0.93636954, 0.93443626, 0.93229055, 0.93012655, 0.95709205, 0.9417728, 0.92976654, 0.9406885, 0.9430094, 0.9470978, 0.93723226, 0.951864, 0.95099205, 0.9500416, 0.9403932], 'recall': [0.839535, 0.8395348, 0.82906973, 0.8395351, 0.8395352, 0.839535, 0.839535, 0.8290699, 0.82907, 0.839535, 0.839535, 0.8395351, 0.8395351, 0.8395351, 0.8395351, 0.83953494, 0.83953494, 0.8395348, 0.839535, 0.8290698, 0.8395351, 0.839535, 0.83953494, 0.83953494, 0.8290699, 0.83953494, 0.83953494, 0.83953494, 0.8395351, 0.8395351, 0.8290698, 0.839535, 0.8395351, 0.8395352, 0.83953494, 0.83953494, 0.83837235, 0.8372094, 0.83720976, 0.8244189, 0.82209337, 0.8313955, 0.83255845, 0.8313956, 0.82906973, 0.8279072, 0.82558155, 0.8244186, 0.82790715, 0.8139536, 0.8267443, 0.8209304, 0.82558167, 0.8220932, 0.82209307, 0.82209307, 0.8209305, 0.823256, 0.80930257, 0.81395346, 0.81860477, 0.80930257, 0.8186048, 0.82209307, 0.8174419, 0.8046512, 0.8127909, 0.8127908, 0.8127908, 0.8023258]}\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 9)                 126       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 70        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 32        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 11)                55        \n",
            "=================================================================\n",
            "Total params: 283\n",
            "Trainable params: 283\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n",
            "(366,)\n",
            "(366, 11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f51fc9cdb38>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAP00lEQVR4nO3df6zddX3H8eeLVmwG+AN7TYC2FLUkNm5RcoMsGmTqlsIf7R9uhiboXFCiG2ZGs4TFhRGMMc7MJZpuWjPjNBEEE81NqBDnMCTGOi6iaCHoFZW2wrg6RBIjpfLeH+d0Pb29t+e0Pfecez73+Uhu+v3x6fm+Pz33vvq5n+/3e76pKiRJk++McRcgSRoOA12SGmGgS1IjDHRJaoSBLkmNWDuuA69fv742b948rsNL0kS67777fllVU4vtG1ugb968mdnZ2XEdXpImUpKfL7XPKRdJaoSBLkmNMNAlqREGuiQ1wkCXpEb0DfQkn03yRJIfLrE/ST6RZC7JA0kuGX6ZkqR+Bhmhfw7YdoL9VwJbul/XAf92+mVJkk5W3+vQq+qeJJtP0GQH8PnqfA7v3iQvSnJeVT02pBqH5sc/hve9D55+etyVSFrN3vhGuOmm4b/uMG4sugDY37N+oLvtuEBPch2dUTybNm0awqFPzu23w549Iz+sJB3j/POX53VHeqdoVe0GdgNMT0+P/Mkav/9958+3vQ3e+c5RH12SOqYWvXH/9A0j0A8CG3vWN3S3rVgXXgiXXz7uKiRpuIZx2eIM8Pbu1S6XAU+txPlzSWpd3xF6kluAK4D1SQ4A/wg8D6CqPgXsAa4C5oDfAn+1XMVKkpY2yFUuO/vsL+BvhlaRJOmUeKeoJDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRqwddwHL6StfgQ9/GA4f7qw//vh465Gk5dR0oH/mM3Dffcdv37Jl9LVI0nJrOtCrOn9+8pPw+td3ls8+G17xivHVJEnLpelAP+LlL4dXv3rcVUjS8vKkqCQ1YqBAT7ItycNJ5pLcsMj+TUnuTnJ/kgeSXDX8UiVJJ9I30JOsAXYBVwJbgZ1Jti5o9g/AbVX1GuBq4F+HXagk6cQGGaFfCsxV1SNVdQi4FdixoE0BL+guvxD4xfBKlCQNYpBAvwDY37N+oLut103ANUkOAHuA9y72QkmuSzKbZHZ+fv4UypUkLWVYJ0V3Ap+rqg3AVcAXkhz32lW1u6qmq2p6ampqSIeWJMFggX4Q2NizvqG7rde1wG0AVfVtYB2wfhgFSpIGM0ig3wtsSXJRkjPpnPScWdDmUeBNAEleSSfQnVORpBHqG+hVdRi4HrgLeIjO1Sz7ktycZHu32QeAdyX5PnAL8I6qI/dpSpJGYaA7RatqD52Tnb3bbuxZfhB43XBLkySdDO8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0YKNCTbEvycJK5JDcs0eatSR5Msi/JF4dbpiSpn7X9GiRZA+wC/hQ4ANybZKaqHuxpswX4e+B1VfVkkpcuV8GSpMUNMkK/FJirqkeq6hBwK7BjQZt3Abuq6kmAqnpiuGVKkvoZJNAvAPb3rB/obut1MXBxkm8l2Ztk22IvlOS6JLNJZufn50+tYknSooZ1UnQtsAW4AtgJfCbJixY2qqrdVTVdVdNTU1NDOrQkCQYL9IPAxp71Dd1tvQ4AM1X1bFX9FPgRnYCXJI3IIIF+L7AlyUVJzgSuBmYWtPkqndE5SdbTmYJ5ZIh1SpL66BvoVXUYuB64C3gIuK2q9iW5Ocn2brO7gF8leRC4G/i7qvrVchUtSTpe38sWAapqD7BnwbYbe5YLeH/3S5I0Bt4pKkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IhmA/3rX4c77xx3FZI0Os0G+h13HF3eunV8dUjSqDQb6Ed85CNw4YXjrkKSll/zgb5u3bgrkKTRaD7QJWm1MNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaMVCgJ9mW5OEkc0luOEG7tySpJNPDK1GSNIi+gZ5kDbALuBLYCuxMctwzgJKcA/wt8J1hFylJ6m+QEfqlwFxVPVJVh4BbgR2LtPsQ8FHgd0OsT5I0oEEC/QJgf8/6ge62/5fkEmBjVd3BCSS5Lslsktn5+fmTLlaStLTTPima5Azg48AH+rWtqt1VNV1V01NTU6d7aElSj0EC/SCwsWd9Q3fbEecArwK+meRnwGXAjCdGJWm0Bgn0e4EtSS5KciZwNTBzZGdVPVVV66tqc1VtBvYC26tqdlkqliQtqm+gV9Vh4HrgLuAh4Laq2pfk5iTbl7tASdJg1g7SqKr2AHsWbLtxibZXnH5ZkqST5Z2iktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREDXbY4KZ59Fi6/HL77XTh8eNzVSNJoNRXoc3Owd+/R9bPPhte+dnz1SNIoNRXojz7a+fOKK+DOO2HNGljbVA8laWlNxd3+7of8bt4Mz3/+WEuRpJFr6qTokRH6xo0nbidJLWoq0I+M0DdtGm8dkjQOTQX6kRG6gS5pNWoy0J1ykbQaNRPoVUenXAx0SatRM4E+Pw/PPAMvfnHn+nNJWm2aCXRPiEpa7ZoJdOfPJa12zQS6I3RJq10zge4IXdJq11ygO0KXtFo1E+hOuUha7ZoJdKdcJK12TQT6s8/CY4/BGWfA+eePuxpJGo8mAv3gwc6douedB8973rirkaTxaCLQnT+XpEYC3flzSWos0B2hS1rNmgh0p1wkqZFAd8pFkgYM9CTbkjycZC7JDYvsf3+SB5M8kOQbSS4cfqlLc4QuSQMEepI1wC7gSmArsDPJ1gXN7gemq+qPgC8D/zTsQk/EEbokDTZCvxSYq6pHquoQcCuwo7dBVd1dVb/tru4FNgy3zKU9/TT8+tewbh2sXz+qo0rSyjNIoF8A7O9ZP9DdtpRrga8ttiPJdUlmk8zOz88PXuUJ9D52LhnKS0rSRBrqSdEk1wDTwMcW219Vu6tquqqmp6amhnJML1mUpI61A7Q5CPTOTm/objtGkjcDHwTeUFXPDKe8/nwwtCR1DDJCvxfYkuSiJGcCVwMzvQ2SvAb4NLC9qp4YfplLc4QuSR19A72qDgPXA3cBDwG3VdW+JDcn2d5t9jHgbOD2JN9LMrPEyw2dlyxKUscgUy5U1R5gz4JtN/Ysv3nIdQ3MSxYlqWPi7xR1hC5JHRMd6M8950lRSTpiogN9fh6eeQbOPRfOOmvc1UjSeE10oDs6l6SjJjrQvWRRko6a6EB3hC5JR010oDtCl6SjJjrQvWRRko6a6ED3piJJOqqJQHeELkkTHOiHDsHjj8MZZ8D554+7Gkkav4kN9IMHoaoT5msH+kQaSWrbxAa6lyxK0rEmNtCdP5ekY01soHvJoiQda2ID3UsWJelYEx/ojtAlqWNiA92TopJ0rIkNdEfoknSsiQz03/wGnnoK1q2Dl7xk3NVI0sowkYHee4VLMt5aJGmlmOhAd/5cko6ayEB3/lySjmegS1IjJjLQnXKRpONNZKA7Qpek401koDtCl6TjTVygP/ecgS5Ji5m4QH/iic7Tis49F846a9zVSNLKMXGB7sfmStLiJi7QPSEqSYsbKNCTbEvycJK5JDcssv/5Sb7U3f+dJJuHXegRzp9L0uL6BnqSNcAu4EpgK7AzydYFza4FnqyqVwD/Anx02IUe4QhdkhY3yAj9UmCuqh6pqkPArcCOBW12AP/RXf4y8KZkeT42yxG6JC1ukEC/ANjfs36gu23RNlV1GHgKOO6DbZNcl2Q2yez8/PwpFbx2LZx9tiN0SVpo7SgPVlW7gd0A09PTdSqvccstUKf0NyWpbYOM0A8CvRMcG7rbFm2TZC3wQuBXwyhwMYmfgy5JCw0S6PcCW5JclORM4GpgZkGbGeAvu8t/DvxXleNoSRqlvlMuVXU4yfXAXcAa4LNVtS/JzcBsVc0A/w58Ickc8L90Ql+SNEIDzaFX1R5gz4JtN/Ys/w74i+GWJkk6GRN3p6gkaXEGuiQ1wkCXpEYY6JLUiIzr6sIk88DPT/Gvrwd+OcRyJoF9Xh3s8+pwOn2+sKqmFtsxtkA/HUlmq2p63HWMkn1eHezz6rBcfXbKRZIaYaBLUiMmNdB3j7uAMbDPq4N9Xh2Wpc8TOYcuSTrepI7QJUkLGOiS1IgVHegr6eHUozJAn9+f5MEkDyT5RpILx1HnMPXrc0+7tySpJBN/idsgfU7y1u57vS/JF0dd47AN8L29KcndSe7vfn9fNY46hyXJZ5M8keSHS+xPkk90/z0eSHLJaR+0qlbkF52P6v0J8DLgTOD7wNYFbf4a+FR3+WrgS+OuewR9/hPgD7rL71kNfe62Owe4B9gLTI+77hG8z1uA+4EXd9dfOu66R9Dn3cB7ustbgZ+Nu+7T7PPlwCXAD5fYfxXwNSDAZcB3TveYK3mEvqIeTj0ifftcVXdX1W+7q3vpPEFqkg3yPgN8CPgo8LtRFrdMBunzu4BdVfUkQFU9MeIah22QPhfwgu7yC4FfjLC+oauqe+g8H2IpO4DPV8de4EVJzjudY67kQB/aw6knyCB97nUtnf/hJ1nfPnd/Fd1YVXeMsrBlNMj7fDFwcZJvJdmbZNvIqlseg/T5JuCaJAfoPH/hvaMpbWxO9ue9r5E+JFrDk+QaYBp4w7hrWU5JzgA+DrxjzKWM2lo60y5X0Pkt7J4kf1hVvx5rVctrJ/C5qvrnJH9M5ylor6qq58Zd2KRYySP0Ffdw6hEYpM8keTPwQWB7VT0zotqWS78+nwO8Cvhmkp/RmWucmfATo4O8zweAmap6tqp+CvyITsBPqkH6fC1wG0BVfRtYR+dDrFo10M/7yVjJgb4aH07dt89JXgN8mk6YT/q8KvTpc1U9VVXrq2pzVW2mc95ge1XNjqfcoRjke/urdEbnJFlPZwrmkVEWOWSD9PlR4E0ASV5JJ9DnR1rlaM0Ab+9e7XIZ8FRVPXZarzjuM8F9zhJfRWdk8hPgg91tN9P5gYbOG347MAf8N/Cycdc8gj7/J/A/wPe6XzPjrnm5+7yg7TeZ8KtcBnyfQ2eq6UHgB8DV4655BH3eCnyLzhUw3wP+bNw1n2Z/bwEeA56l8xvXtcC7gXf3vMe7uv8ePxjG97W3/ktSI1bylIsk6SQY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakR/wdCCYXUxknsQgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQPDFVkEevLA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}